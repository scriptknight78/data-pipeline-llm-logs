# Snowflake Data Pipeline for LLM Logs

This repository demonstrates how to create a database in Snowflake, load raw JSON data from a staged file into a table, and process it to create structured data for logs related to large language model (LLM) operations.

## Table of Contents
- [Overview](#overview)
- [Steps in the Code](#steps-in-the-code)
  - [Step 1: Create Durable Database](#step-1-create-durable-database)
  - [Step 2: Create Staging Area](#step-2-create-staging-area)
  - [Step 3: Create Raw Data Table](#step-3-create-raw-data-table)
  - [Step 4: Load Raw Data into Table](#step-4-load-raw-data-into-table)
  - [Step 5: Create Schema for Logs](#step-5-create-schema-for-logs)
  - [Step 6: Create and Transform LLM Logs Table](#step-6-create-and-transform-llm-logs-table)
  - [Step 7: Query LLM Logs Table](#step-7-query-llm-logs-table)
- [Usage](#usage)

## Overview

This code provides an end-to-end solution for loading and processing logs generated by large language model (LLM) operations. The logs are stored in JSON format, and through a series of SQL commands in Snowflake, the data is transformed into a structured format for easier querying and analysis.

## Steps in the Code

### Step 1: Create Durable Database

```
CREATE DATABASE DURABLE_DB;
```
This command creates a new Snowflake database named DURABLE_DB, which will be used to store the data.

### Step 2: Create Staging Area
```
CREATE OR REPLACE STAGE staging
  FILE_FORMAT = (TYPE = 'JSON');
```
A staging area named staging is created. The FILE_FORMAT is set to JSON, which means that files in this staging area are expected to be in JSON format.

### Step 3: Create Raw Data Table
```
CREATE OR REPLACE TABLE raw_data (
  data VARIANT
);
```
This step creates a table called raw_data, which will store the raw JSON data in a column named data with a VARIANT datatype to accommodate different structures.

### Step 4: Load Raw Data into Table
```
COPY INTO raw_data
  FROM @~/staging/llm-logs.json
  FILE_FORMAT = (TYPE = 'JSON');
```
The COPY INTO command is used to load the raw data from the llm-logs.json file located in the staging area into the raw_data table. The data is in JSON format.

### Step 5: Create Schema for Logs
```
CREATE SCHEMA LOGS;
```
A schema named LOGS is created to organize the logs and any associated transformations.

### Step 6: Create and Transform LLM Logs Table
```
CREATE OR REPLACE TABLE LLM_LOGS AS
SELECT 
    TO_TIMESTAMP(f.value:created::STRING) AS created,
    f.value:max_tokens::INTEGER AS max_tokens,
    f.value:metrics.completion_tokens::INTEGER AS completion_tokens,
    TO_TIMESTAMP(f.value:metrics.end::INTEGER) AS end_time,
    f.value:metrics.prompt_tokens::INTEGER AS prompt_tokens,
    TO_TIMESTAMP(f.value:metrics.start::INTEGER) AS start_time,
    f.value:metrics.time_to_first_token::FLOAT AS time_to_first_token,
    f.value:metrics.tokens::INTEGER AS total_tokens,
    f.value:model::STRING AS model,
    f.value:stream::BOOLEAN AS stream,
    f.value:temperature::FLOAT AS temperature,
    f.value:type::STRING AS type
FROM RAW_DATA,
LATERAL FLATTEN(input => RAW_DATA.data:data) AS f;
```
The LLM_LOGS table is created by transforming the raw JSON data into a structured format. Using the LATERAL FLATTEN function, the JSON data is parsed, and specific fields (e.g., created, max_tokens, model, etc.) are extracted and cast into the appropriate data types such as INTEGER, STRING, FLOAT, and BOOLEAN.

### Step 7: Query LLM Logs Table
```
SELECT * FROM LLM_LOGS;
```
Finally, the SELECT statement retrieves all the records from the LLM_LOGS table, providing the processed and structured log data.

Usage
To use this code, follow these steps:

- Set up your Snowflake account and ensure you have the necessary permissions to create databases, stages, tables, and schemas.
- Upload your raw llm-logs.json file to the staging area in Snowflake.
- Execute the SQL commands in the correct order to load and transform the data.
- Run the SELECT queries to analyze the structured LLM logs.
